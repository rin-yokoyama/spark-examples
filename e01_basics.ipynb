{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c77ccc-27e8-4c94-bc18-68eb3423d583",
   "metadata": {},
   "source": [
    "# Basics\n",
    "このノートでは、pysparkを用いてデータフレームを基本的な操作する例を示した。\n",
    "\n",
    "## SparkSessionを立てる\n",
    "pythonからpysparkを使う時には、SparkSession.builder.getOrCreate()でセッションを立てる。\n",
    "ローカルで実行する際にはこれでとりあえず使える。\n",
    "\n",
    "## リソース管理\n",
    "このままだと全てのCPU coreを使うので、制限したい場合はmaster(\"local[4]\")のように指定する。\n",
    "config(\"spark.driver.memory\",\"2g\")のようにセッションの設定値を指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e07707",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/08 15:19:41 WARN Utils: Your hostname, TABLET-S9I8ER9S, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/08 15:19:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/08 15:19:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local[4]\") \\\n",
    "#                            .config(\"spark.driver.memory\",\"2g\") \\\n",
    "#                            .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235d24d-3eb3-4365-a92b-ce6c5d0c3678",
   "metadata": {},
   "source": [
    "## Parquetファイルを開く\n",
    "- spark.read.parquet(): parquetファイルを開いてデータフレームを作成する。\n",
    "- df: データフレーム\n",
    "- df.show(): データフレームを表示。引数で行数指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45a8028-4018-42cb-97d5-0226bcb310c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+-----+----+\n",
      "|ts           |ch |cfd  |adc |\n",
      "+-------------+---+-----+----+\n",
      "|1657567439657|8  |23504|1348|\n",
      "|1657568832534|6  |5435 |6750|\n",
      "|1657569292316|2  |12850|744 |\n",
      "|1657569301206|7  |14372|432 |\n",
      "|1657569615821|11 |11872|900 |\n",
      "|1657569880689|6  |31959|838 |\n",
      "|1657569905859|6  |28153|344 |\n",
      "|1657569962320|10 |4468 |552 |\n",
      "|1657570040927|0  |31680|2531|\n",
      "|1657570187406|2  |22049|456 |\n",
      "+-------------+---+-----+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./data/clover_data.parquet\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3beced-2129-42d3-970e-0c6900122d5c",
   "metadata": {},
   "source": [
    "## Schemaの表示\n",
    "printSchema()でデータフレームの構造を表示させることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ebb226-b27f-41a6-ab0d-b8dffeb8673a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- ch: short (nullable = true)\n",
      " |-- cfd: short (nullable = true)\n",
      " |-- adc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec1691-e341-4161-acaf-c22f8b7863e2",
   "metadata": {},
   "source": [
    "## その他関数\n",
    "- count(): 行数をカウントする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f959b1-7c94-412e-b916-322a4459a0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858760"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e841c-d230-47c8-ae58-04073d704e79",
   "metadata": {},
   "source": [
    "- select(): 指定した列だけ取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d935d632-8aa5-4c27-8ad4-8c2f411ad763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| ch| adc|           ts|\n",
      "+---+----+-------------+\n",
      "|  8|1348|1657567439657|\n",
      "|  6|6750|1657568832534|\n",
      "|  2| 744|1657569292316|\n",
      "|  7| 432|1657569301206|\n",
      "| 11| 900|1657569615821|\n",
      "+---+----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"ch\",\"adc\",\"ts\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1bf7-ae9d-45db-8ca2-49de8f7a337d",
   "metadata": {},
   "source": [
    "- filter(): 条件を満たす行だけ取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc14c4-f512-41e4-b3fe-b907026ba210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| ch| adc|           ts|\n",
      "+---+----+-------------+\n",
      "|  2| 744|1657569292316|\n",
      "|  0|2531|1657570040927|\n",
      "|  2| 456|1657570187406|\n",
      "|  0|   0|1657570307668|\n",
      "|  3|3714|1657570429207|\n",
      "+---+----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(\"ch>=0 AND ch<4\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507008c-f15a-4788-b7b5-8bab366a9b49",
   "metadata": {},
   "source": [
    "## spark.sql.functions\n",
    "\n",
    "様々なpyspark SQL関数が定義されている。以降、`F`としてインポートしておく\n",
    "- F.col(): データフレームの列を返す\n",
    "- F.lit(): 定数からなる列を返す\n",
    "- withColumn(): 列を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d633521-ad07-430e-a58a-2e4c6821e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+------------------+\n",
      "| ch| adc|           ts|         ts_in_sec|\n",
      "+---+----+-------------+------------------+\n",
      "|  2| 744|1657569292316|1657.5692923160002|\n",
      "|  0|2531|1657570040927|    1657.570040927|\n",
      "|  2| 456|1657570187406|    1657.570187406|\n",
      "|  0|   0|1657570307668|1657.5703076680002|\n",
      "|  3|3714|1657570429207|1657.5704292070002|\n",
      "+---+----+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# ts (ns)をsecに変換した列を定義\n",
    "df.withColumn(\"ts_in_sec\", F.lit(1.e-9)*F.col(\"ts\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d2233-41cb-45d3-861e-b0fa6dbe99cd",
   "metadata": {},
   "source": [
    "- F.expr(): データフレームの列に対してSQL表記の操作を行う。\n",
    "\n",
    "上のセルのような操作を完結に表記できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703a692c-10f6-49ff-a8ec-27e979d27f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+------------------+\n",
      "| ch| adc|           ts|         ts_in_sec|\n",
      "+---+----+-------------+------------------+\n",
      "|  2| 744|1657569292316|1657.5692923160002|\n",
      "|  0|2531|1657570040927|    1657.570040927|\n",
      "|  2| 456|1657570187406|    1657.570187406|\n",
      "|  0|   0|1657570307668|1657.5703076680002|\n",
      "|  3|3714|1657570429207|1657.5704292070002|\n",
      "+---+----+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ts_in_sec\", F.expr(\"1.e-9*ts\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19bac-9bdb-4304-9fb8-860673c800e5",
   "metadata": {},
   "source": [
    "- spark.sql(): SQLコマンドを実行する。\n",
    "- df.createOrReplaceTempView(): データフレームをSQLで呼ぶためのTempViewに登録する。\n",
    "\n",
    "下はSQLで\"ch\"毎の\"adc\"の値の平均をとる操作を行うもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f681ef07-76a8-43d5-8f93-4687e401da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| ch|           avg_adc|\n",
      "+---+------------------+\n",
      "|  1|1360.1958049005211|\n",
      "|  3|3497.6671999375685|\n",
      "|  2|1210.0397673898456|\n",
      "|  0|3590.5568891736652|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT ch, AVG(adc) AS avg_adc\n",
    "    FROM df\n",
    "    GROUP BY ch\n",
    "\"\"\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
