{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c77ccc-27e8-4c94-bc18-68eb3423d583",
   "metadata": {},
   "source": [
    "# Basics\n",
    "このノートでは、pysparkを用いてデータフレームを基本的な操作する例を示した。\n",
    "\n",
    "## SparkSessionを立てる\n",
    "pythonからpysparkを使う時には、SparkSession.builder.getOrCreate()でセッションを立てる。\n",
    "ローカルで実行する際にはこれでとりあえず使える。\n",
    "\n",
    "## リソース管理\n",
    "このままだと全てのCPU coreを使うので、制限したい場合はmaster(\"local[4]\")のように指定する。\n",
    "config(\"spark.driver.memory\",\"2g\")のようにセッションの設定値を指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e07707",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/08 17:14:03 WARN Utils: Your hostname, TABLET-S9I8ER9S, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/08 17:14:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/08 17:14:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local[4]\") \\\n",
    "#                            .config(\"spark.driver.memory\",\"2g\") \\\n",
    "#                            .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235d24d-3eb3-4365-a92b-ce6c5d0c3678",
   "metadata": {},
   "source": [
    "## Parquetファイルを開く\n",
    "- spark.read.parquet(): parquetファイルを開いてデータフレームを作成する。\n",
    "- df: データフレーム\n",
    "- df.show(): データフレームを表示。引数で行数指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45a8028-4018-42cb-97d5-0226bcb310c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+-----+----+\n",
      "|ts           |ch |cfd  |adc |\n",
      "+-------------+---+-----+----+\n",
      "|7296321907778|4  |8485 |3982|\n",
      "|7296321978243|2  |9278 |774 |\n",
      "|7296321997358|1  |8225 |1760|\n",
      "|7296322069363|0  |16234|0   |\n",
      "|7296322214400|4  |24255|4526|\n",
      "|7296322271040|1  |32358|751 |\n",
      "|7296322391382|4  |26343|1315|\n",
      "|7296322439661|0  |21550|0   |\n",
      "|7296322447462|7  |3413 |245 |\n",
      "|7296322574024|2  |12205|3798|\n",
      "+-------------+---+-----+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./data/clover_data.parquet\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3beced-2129-42d3-970e-0c6900122d5c",
   "metadata": {},
   "source": [
    "## Schemaの表示\n",
    "printSchema()でデータフレームの構造を表示させることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ebb226-b27f-41a6-ab0d-b8dffeb8673a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- ch: short (nullable = true)\n",
      " |-- cfd: short (nullable = true)\n",
      " |-- adc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec1691-e341-4161-acaf-c22f8b7863e2",
   "metadata": {},
   "source": [
    "## その他関数\n",
    "- count(): 行数をカウントする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f959b1-7c94-412e-b916-322a4459a0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521829"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e841c-d230-47c8-ae58-04073d704e79",
   "metadata": {},
   "source": [
    "- select(): 指定した列だけ取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d935d632-8aa5-4c27-8ad4-8c2f411ad763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| ch| adc|           ts|\n",
      "+---+----+-------------+\n",
      "|  4|3982|7296321907778|\n",
      "|  2| 774|7296321978243|\n",
      "|  1|1760|7296321997358|\n",
      "|  0|   0|7296322069363|\n",
      "|  4|4526|7296322214400|\n",
      "+---+----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"ch\",\"adc\",\"ts\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1bf7-ae9d-45db-8ca2-49de8f7a337d",
   "metadata": {},
   "source": [
    "- filter(): 条件を満たす行だけ取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc14c4-f512-41e4-b3fe-b907026ba210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| ch| adc|           ts|\n",
      "+---+----+-------------+\n",
      "|  2| 774|7296321978243|\n",
      "|  1|1760|7296321997358|\n",
      "|  0|   0|7296322069363|\n",
      "|  1| 751|7296322271040|\n",
      "|  0|   0|7296322439661|\n",
      "+---+----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(\"ch>=0 AND ch<4\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507008c-f15a-4788-b7b5-8bab366a9b49",
   "metadata": {},
   "source": [
    "## spark.sql.functions\n",
    "\n",
    "様々なpyspark SQL関数が定義されている。以降、`F`としてインポートしておく\n",
    "- F.col(): データフレームの列を返す\n",
    "- F.lit(): 定数からなる列を返す\n",
    "- withColumn(): 列を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d633521-ad07-430e-a58a-2e4c6821e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+------------------+\n",
      "| ch| adc|           ts|         ts_in_sec|\n",
      "+---+----+-------------+------------------+\n",
      "|  2| 774|7296321978243| 7296.321978243001|\n",
      "|  1|1760|7296321997358| 7296.321997358001|\n",
      "|  0|   0|7296322069363|    7296.322069363|\n",
      "|  1| 751|7296322271040|7296.3222710400005|\n",
      "|  0|   0|7296322439661|    7296.322439661|\n",
      "+---+----+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# ts (ns)をsecに変換した列を定義\n",
    "df.withColumn(\"ts_in_sec\", F.lit(1.e-9)*F.col(\"ts\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d2233-41cb-45d3-861e-b0fa6dbe99cd",
   "metadata": {},
   "source": [
    "- F.expr(): データフレームの列に対してSQL表記の操作を行う。\n",
    "\n",
    "上のセルのような操作を完結に表記できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703a692c-10f6-49ff-a8ec-27e979d27f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+------------------+\n",
      "| ch| adc|           ts|         ts_in_sec|\n",
      "+---+----+-------------+------------------+\n",
      "|  2| 774|7296321978243| 7296.321978243001|\n",
      "|  1|1760|7296321997358| 7296.321997358001|\n",
      "|  0|   0|7296322069363|    7296.322069363|\n",
      "|  1| 751|7296322271040|7296.3222710400005|\n",
      "|  0|   0|7296322439661|    7296.322439661|\n",
      "+---+----+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ts_in_sec\", F.expr(\"1.e-9*ts\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19bac-9bdb-4304-9fb8-860673c800e5",
   "metadata": {},
   "source": [
    "- spark.sql(): SQLコマンドを実行する。\n",
    "- df.createOrReplaceTempView(): データフレームをSQLで呼ぶためのTempViewに登録する。\n",
    "\n",
    "下はSQLで\"ch\"毎の\"adc\"の値の平均をとる操作を行うもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f681ef07-76a8-43d5-8f93-4687e401da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| ch|           avg_adc|\n",
      "+---+------------------+\n",
      "|  1|2041.8029956882683|\n",
      "|  3| 5516.067150776918|\n",
      "|  2|1937.2271148371594|\n",
      "|  0| 4341.374239243097|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT ch, AVG(adc) AS avg_adc\n",
    "    FROM df\n",
    "    GROUP BY ch\n",
    "\"\"\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
