{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95500194-eb51-4f15-8642-775655d5f4da",
   "metadata": {},
   "source": [
    "# Energy calibration\n",
    "このノートでは、検出器のraw ADCの値をch毎に線形関数でキャリブレーションし、エネルギー情報に変換する。\n",
    "キャリブレーションパラメータはCSVファイルで準備済みとする。\n",
    "\n",
    "- data/clover_data.parquet:\n",
    "こちらは、クローバー型ゲルマニウム検出器2台からなるデータで、各検出器に4つの読み出しチャンネルがあり、\n",
    "合計8chとなっている。\n",
    "\n",
    "- ClvCalib.csv: キャリブレーションパラメータ\n",
    "\n",
    "## ファイルを開く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72423407-0b98-4fca-9583-5aae20fafcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/08 17:14:54 WARN Utils: Your hostname, TABLET-S9I8ER9S, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/08 17:14:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/08 17:14:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+----+\n",
      "|           ts|  cfd| ch| adc|\n",
      "+-------------+-----+---+----+\n",
      "|7296321907778| 8485|  4|3982|\n",
      "|7296321978243| 9278|  2| 774|\n",
      "|7296321997358| 8225|  1|1760|\n",
      "|7296322069363|16234|  0|   0|\n",
      "|7296322214400|24255|  4|4526|\n",
      "+-------------+-----+---+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet(\"./data/clover_data.parquet\")\n",
    "df_select = df.select(\"ts\",\"cfd\",\"ch\",\"adc\")\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d575e5-1f04-4dcc-bfc6-1aeca2bc1855",
   "metadata": {},
   "source": [
    "とりあえず\"ch\"と\"adc\"列だけセレクト\n",
    "## 整数値を実数値に変換する。\n",
    "raw ADCの値は整数値なので、0から1の乱数を足して実数値にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78068b3-53ab-4b9d-a8e4-7cbb5e5e5ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+----+------------------+\n",
      "|           ts|  cfd| ch| adc|          adc_real|\n",
      "+-------------+-----+---+----+------------------+\n",
      "|7296321907778| 8485|  4|3982| 3982.394205589545|\n",
      "|7296321978243| 9278|  2| 774| 774.3927821492072|\n",
      "|7296321997358| 8225|  1|1760|1760.7673300781569|\n",
      "|7296322069363|16234|  0|   0|0.8418421262645442|\n",
      "|7296322214400|24255|  4|4526| 4526.628214969451|\n",
      "+-------------+-----+---+----+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_real = df_select.withColumn(\"adc_real\", F.col(\"adc\")+F.rand())\n",
    "df_real.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64113b65-6a8c-48b7-a2e4-3aa3980e03dc",
   "metadata": {},
   "source": [
    "## CSVからキャリブレーションパラメータのデータフレームを作る\n",
    "CSVファイルがあるとする。\n",
    "```\n",
    "ch,p0,p1\n",
    "0,0.2149837759,0.0803461538\n",
    "1,0.1280723048,0.2546962000\n",
    "2,-0.1933825665,0.2941007359\n",
    "3,0.0611566264,0.0929934353\n",
    "4,-0.0563791084,0.2946970847\n",
    "5,0.0371477634, 0.1212543693\n",
    "6,-0.3015558027966563,0.29948038289716816\n",
    "7,-0.09316673031071332,0.28713228807294117\n",
    "```\n",
    "spark.read.csv() でCSVからデータフレームを作成。\n",
    "`header=True`は一行目を列の名前として読むオプション。\n",
    "`inferSchema=True`はデータをstringではなく、推論した型で読み込むオプション。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647b358d-9fc4-48ef-aba3-819d11c6eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| ch|                  p0|                 p1|\n",
      "+---+--------------------+-------------------+\n",
      "|  0|        0.2149837759|       0.0803461538|\n",
      "|  1|        0.1280723048|          0.2546962|\n",
      "|  2|       -0.1933825665|       0.2941007359|\n",
      "|  3|        0.0611566264|       0.0929934353|\n",
      "|  4|       -0.0563791084|       0.2946970847|\n",
      "|  5|        0.0371477634|       0.1212543693|\n",
      "|  6| -0.3015558027966563|0.29948038289716816|\n",
      "|  7|-0.09316673031071332|0.28713228807294117|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_df = spark.read.csv(\"./ClvCalib.csv\", header=True, inferSchema=True)\n",
    "param_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469c491-811c-47bd-92b4-0ee634f34b8d",
   "metadata": {},
   "source": [
    "## ch番号が一致するパラメータを元のデータフレームにくっつける。\n",
    "- join(): 二つのデータフレームを`on=`で指定した条件を満たす行同士で結合する。`how=\"inner\"`は両方のデータフレームの行が存在するものだけ出力、`how=\"outer\"`は片方が存在しない行も残す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8e3c48-dbc0-4ee8-bc94-ce22e3621ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+----+------------------+-------------+------------+\n",
      "| ch|           ts|  cfd| adc|          adc_real|           p0|          p1|\n",
      "+---+-------------+-----+----+------------------+-------------+------------+\n",
      "|  4|7296321907778| 8485|3982| 3982.394205589545|-0.0563791084|0.2946970847|\n",
      "|  2|7296321978243| 9278| 774| 774.3927821492072|-0.1933825665|0.2941007359|\n",
      "|  1|7296321997358| 8225|1760|1760.7673300781569| 0.1280723048|   0.2546962|\n",
      "|  0|7296322069363|16234|   0|0.8418421262645442| 0.2149837759|0.0803461538|\n",
      "|  4|7296322214400|24255|4526| 4526.628214969451|-0.0563791084|0.2946970847|\n",
      "+---+-------------+-----+----+------------------+-------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "calib_df = df_real.join(param_df, on=[\"ch\"], how=\"inner\")\n",
    "calib_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aff7ee-a2dc-4537-a43b-4f2c5215cab9",
   "metadata": {},
   "source": [
    "p0、p1の列に、ch番号に対応する値が入った。\n",
    "## パラメータを使ってエネルギーに変換\n",
    "ここまでできればあとは新しくadc_calを定義するだけ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3986177-4b93-4a78-b676-df5ab3d58830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+----+------------------+-------------+------------+------------------+\n",
      "| ch|           ts|  cfd| adc|          adc_real|           p0|          p1|           adc_cal|\n",
      "+---+-------------+-----+----+------------------+-------------+------------+------------------+\n",
      "|  4|7296321907778| 8485|3982| 3982.394205589545|-0.0563791084|0.2946970847|1173.5435834050113|\n",
      "|  2|7296321978243| 9278| 774| 774.3927821492072|-0.1933825665|0.2941007359|227.55610453923023|\n",
      "|  1|7296321997358| 8225|1760|1760.7673300781569| 0.1280723048|   0.2546962|448.58882035985226|\n",
      "|  0|7296322069363|16234|   0|0.8418421262645442| 0.2149837759|0.0803461538|0.2826225528521701|\n",
      "|  4|7296322214400|24255|4526| 4526.628214969451|-0.0563791084|0.2946970847| 1333.927759363862|\n",
      "+---+-------------+-----+----+------------------+-------------+------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "calib_df = calib_df.withColumn(\"adc_cal\", F.expr(\"p0 + adc_real * p1\"))\n",
    "calib_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c78c8-0e07-4c25-a53d-c9f2e56cf28e",
   "metadata": {},
   "source": [
    "## ファイルに書き出す\n",
    "- write.parquet(): parquetファイルにデータフレームを書き出す。\n",
    "- mode(\"overwrite\"): ファイルが存在する場合、上書きする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3df68e-6911-4184-82e4-c27d070e38da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "calib_df.select(\"ts\",\"ch\",\"cfd\",\"adc_cal\").write.mode(\"overwrite\").parquet(\"./data/clover_caldata.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
