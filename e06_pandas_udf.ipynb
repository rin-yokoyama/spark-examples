{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67b121b-ad87-4ebf-9468-8850fe947920",
   "metadata": {},
   "source": [
    "## pandas UDF\n",
    "pyspark UDFの問題点として、Sparkの走っているJVMとpythonの間で行毎のデータ移動が発生し、パフォーマンスが低くなることが挙げられる。<br>\n",
    "そこで登場したpandas UDFは、データをApache Arrowを用いて言語間データ利用を効率化し、pythonのデータフレーム解析ライブラリであるPandasを用いてデータのベクトル操作が可能になる。<br>\n",
    "\n",
    "このノートでは、検出器信号の波形データの操作を例として示す。<br>\n",
    "後半でscipyを使用するので、\n",
    "```\n",
    "conda install scipy\n",
    "```\n",
    "でインストールしておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0eaaa0c-73ea-4941-96e1-a97e9f05a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/10 17:25:21 WARN Utils: Your hostname, TABLET-S9I8ER9S, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/10 17:25:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/10 17:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|event_id|            waveform|\n",
      "+--------+--------------------+\n",
      "|       0|[-10.649999999999...|\n",
      "|       1|[4.29999999999927...|\n",
      "|       2|[-6.4500000000007...|\n",
      "|       3|[6.95000000000072...|\n",
      "|       4|[-18.700000000000...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"./data/sample_waveform.parquet\").select(\"event_id\",\"waveform\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67751a2c-db49-4436-b83f-8aeb27d6e8a7",
   "metadata": {},
   "source": [
    "このデータには、\"waveform\"列に波形データがアレイとして格納されている。<br>\n",
    "まずは例として、waveformの最初の20点をベースラインとし、その標準偏差を返すpandas UDFを定義してみる。\n",
    "\n",
    "pandas UDFを定義するには、@pandas_udf(戻り値の型)というデコレータを関数の前に付ける。<br>\n",
    "引数・戻り値ともにデータフレームの列である場合pandas.Series型を指定する。<br>\n",
    "以下がUDFの定義部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c27c3bb-b510-4491-ba94-2ca12c5f69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def baseline_std_udf(waveform: pd.Series) -> pd.Series:\n",
    "    return waveform.apply(lambda x: float(np.std(x[:20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95382487-4d08-4327-a329-ce59c2d56b2d",
   "metadata": {},
   "source": [
    "これによって、pyspark関数内でbaseline_std_udfが使えるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab309a8-ae75-4255-a361-2a1925b2845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+\n",
      "|event_id|            waveform|      baseline_std|\n",
      "+--------+--------------------+------------------+\n",
      "|       0|[-10.649999999999...|12.146089905809195|\n",
      "|       1|[4.29999999999927...|13.697079980784226|\n",
      "|       2|[-6.4500000000007...|   8.3694384518915|\n",
      "|       3|[6.95000000000072...|10.552132485900659|\n",
      "|       4|[-18.700000000000...|11.243220179290274|\n",
      "|       5|[7.04999999999927...|12.395462879618492|\n",
      "|       6|[5.75, -7.25, -2....|  6.94172168845741|\n",
      "|       7|[-22.399999999999...|23.438003327928765|\n",
      "|       8|[-2.25, -6.25, 7....|  9.33742469849155|\n",
      "|       9|[0.70000000000072...|12.284543133547945|\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.withColumn(\"baseline_std\", baseline_std_udf(df.waveform)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556f626-005a-44c8-bcbc-5c09185326b9",
   "metadata": {},
   "source": [
    "\"baseline_std\"列に波形のベースライン部分の標準偏差が作られた。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23deb7e0-205a-4b2f-b22c-42593111b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RISETIME=1.8\n",
    "DECAYTIME=84\n",
    "TOFFSET=30\n",
    "# Fit 関数の定義\n",
    "def model(x, A, t0):\n",
    "    return A * (1+np.erf((x-t0)/RISETIME))*0.5*np.exp(-(x-t0)/DECAYTIME)\n",
    "    \n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "# 返り値のデータフレームのSchema\n",
    "schema = StructType([\n",
    "    StructField(\"amplitude\", DoubleType()),\n",
    "    StructField(\"t0\", DoubleType()),\n",
    "])\n",
    "\n",
    "# Scipy curve_fitをするUDFの定義\n",
    "@pandas_udf(schema)\n",
    "def fit_waveforms(wave: pd.Series) -> pd.DataFrame:\n",
    "    def fit_wave(y):\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        x = np.arange(len(y), dtype=float)\n",
    "        popt, _ = curve_fit(\n",
    "            model, x, y,\n",
    "            p0=[y.max, TOFFSET],\n",
    "            bounds=([1e-6, 0], [np.inf,np.inf])\n",
    "        )\n",
    "        return [float(popt[0]), float(popt[1])]\n",
    "\n",
    "    return pd.DataFrame(wave.apply(fit_wave).tolist(), columns=[\"amplitude\",\"t0\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea73bd10-7cad-4508-a53c-ef18ee497250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 18:09:57 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_326921/1393252530.py\", line 28, in fit_waveforms\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/series.py\", line 4943, in apply\n",
      "    ).apply()\n",
      "      ^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1422, in apply\n",
      "    return self.apply_standard()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/base.py\", line 925, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas/_libs/lib.pyx\", line 2999, in pandas._libs.lib.map_infer\n",
      "  File \"/tmp/ipykernel_326921/1393252530.py\", line 21, in fit_wave\n",
      "NameError: name 'curve_fit' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:117)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/10 18:09:57 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_326921/1393252530.py\", line 28, in fit_waveforms\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/series.py\", line 4943, in apply\n",
      "    ).apply()\n",
      "      ^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1422, in apply\n",
      "    return self.apply_standard()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/base.py\", line 925, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas/_libs/lib.pyx\", line 2999, in pandas._libs.lib.map_infer\n",
      "  File \"/tmp/ipykernel_326921/1393252530.py\", line 21, in fit_wave\n",
      "NameError: name 'curve_fit' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:117)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/10 18:09:57 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "[Stage 3:>                                                          (0 + 0) / 1]"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_326921/1393252530.py\", line 28, in fit_waveforms\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/series.py\", line 4943, in apply\n    ).apply()\n      ^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1422, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n    mapped = obj._map_values(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/base.py\", line 925, in _map_values\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n    return lib.map_infer(values, mapper, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2999, in pandas._libs.lib.map_infer\n  File \"/tmp/ipykernel_326921/1393252530.py\", line 21, in fit_wave\nNameError: name 'curve_fit' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df2 = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mfit_params\u001b[39m\u001b[33m\"\u001b[39m, fit_waveforms(F.col(\u001b[33m\"\u001b[39m\u001b[33mwaveform\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_params.*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:316\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    309\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m         messageParameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m         },\n\u001b[32m    314\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_326921/1393252530.py\", line 28, in fit_waveforms\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/series.py\", line 4943, in apply\n    ).apply()\n      ^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1422, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n    mapped = obj._map_values(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/base.py\", line 925, in _map_values\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rin/opt/miniconda3/envs/spark4/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n    return lib.map_infer(values, mapper, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2999, in pandas._libs.lib.map_infer\n  File \"/tmp/ipykernel_326921/1393252530.py\", line 21, in fit_wave\nNameError: name 'curve_fit' is not defined\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"fit_params\", fit_waveforms(F.col(\"waveform\")))\n",
    "df2.select(\"fit_params.*\").show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
